\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[left=1in, right=1in, top=1in, bottom=1in]{geometry}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsfonts}
\usepackage{changepage}
\usepackage{amsthm}
\usepackage{graphicx}
% \usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage[backend=bibtex]{biblatex}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\allowdisplaybreaks
\title{PCA as Gradient Descent}
\author{Andrew Draganov}

\begin{document}
\maketitle
\section{PCA objective function and gradient}

Given a high-dimensional dataset $X \in \mathbb{R}^{n \times D}$ and randomly initialized $Y \in \mathbb{R}^{n \times d}$, we aim to approximate a gradient
$\nabla_Y$ that is minimized when $Y$ is the PCA embedding of $X$ into $d$ dimensions.

We start with the claim that PCA minimizes the function
\begin{equation}
    f_{pca}(X, Y) = ||C (E^X - E^Y) C ||^2_F
\end{equation}
where $E^X \in \mathbb{R}^{n \times n}$ and $E^Y \in \mathbb{R}^{n \times n}$ are the pairwise squared distance matrices for $X$ and $Y$ and $C$ is the $n \times
n$ centering matrix.

Before developing its gradient, we first establish the following preliminary:
\begin{definition}{\textit{$E^X$ in terms of Gram matrices ---}}
    Given $X \in \mathbb{R}^{n \times D}$ and Gram matrix $G_X = X X^T$, we can write $E^X$ as $E^X = \text{diag}(G_X) \mathbbm{1}^T + \mathbbm{1}
    \text{diag}(G_X)^T - 2G$ where $\mathbbm{1} \in \mathbb{R}^{n \times D}$ is the
    matrix of all 1's
\end{definition}

We simplify notation by writing $\text{diag}(G_X) = g_x$, giving us \[ E^X = g_X \mathbbm{1}^T + \mathbbm{1} g_X^T - 2G_X \]
By applying this definition, we rearrange our cost function by
\begin{align*}
    f_{pca}(X, Y) &= ||C(E^X - E^Y)C||_F^2 \\
    f_{pca}(X, Y) &= ||C(g_X \mathbbm{1} + \mathbbm{1} g_X^T - 2G_X - g_Y \mathbbm{1} - \mathbbm{1} g_Y^T + 2G_Y)C||_F^2 
\end{align*}
But now notice that $C \mathbbm{1} = 0 = \mathbbm{1}^T C$. So we can simplify this expression into
\[ f_{pca}(X, Y) = ||C(2G_Y - 2G_X)C||_F^2 \]
\textit{We will temporarily ignore the factor $2$ scalar and centering matrices for the purposes of constructing a minimal viable example.} Given these
simplifications, our modified cost function is written as
\[ g_{pca}(X, Y) = ||G_X - G_Y||_F^2 \]
The gradient of this with respect to $Y$ along dimension $\alpha$ is then
\[
\nabla^{\alpha}_Y g_{pca}(X, Y) = 2(G_X - G_Y)(Y_{*\alpha} \mathbbm{1}^T - \mathbbm{1} Y_{*\alpha}^T) \mathbbm{1}
\]
where $Y_{*\alpha}$ is the column of $Y$ along dimension $\alpha$ and $\mathbbm{1} \in \mathbb{R}^{n \times 1}$ is the all-1 vector. Since the dimensions of $Y$
are treated independently, WLOG $d=1$:
\begin{equation} \label{simple_gradient}
    \nabla g_{pca}(X, Y) = 2(G_X - G_Y)(Y \mathbbm{1}^T - \mathbbm{1} Y^T) \mathbbm{1}
\end{equation}

It is this gradient that we will try to approximate with a low-rank substitute.

In the next section we show that $(Y \mathbbm{1}^T - \mathbbm{1} Y^T) \mathbbm{1}$ is just $nCY$ and can thus be computed in $O(n)$ time. If we are given
rank-$k$ approximations for $G_X$ and $G_Y$, we will then be able to calculate the gradient in $O(kn) + O(nd)$ time.

Our principle goal, then, is to determine what effect low-rank approximations $G'_X$ and $G'_Y$ will have on the gradient. Namely, we want low-rank
approximations such that the approximate gradient
\[ \nabla' g_{pca}(X, Y) = 2(G'_X - G'_Y) (Y \mathbbm{1}^T - \mathbbm{1} Y^T) \mathbbm{1} \]
has the property that
\[ \dfrac{\langle \nabla , \nabla' \rangle}{||\nabla|| \cdot ||\nabla'||} \]
is maximized.

\subsection{Simplifying $(Y \mathbbm{1}^T - \mathbbm{1} Y^T) \mathbbm{1}$}
We start by working with the $(Y \mathbbm{1}^T - \mathbbm{1} Y^T) \mathbbm{1}$ term, which we distribute into $Y \mathbbm{1}^T \mathbbm{1} - \mathbbm{1} Y^T
\mathbbm{1}$. Clearly, the first $\mathbbm{1}^T \mathbbm{1}$ term is just $n$. The $\mathbbm{1} Y^T \mathbbm{1}$ is similarly simple, as it is just the sum $\sum_{i=1}^n y_{i}$ in every cell. Their difference, then, can be written as

\begin{equation} \label{scalar_matrix}
(Y \mathbbm{1}^T - \mathbbm{1} Y^T) \mathbbm{1} = 
\begin{bmatrix}
    n \cdot y_1 - \sum_i^n y_i & \dots & n \cdot y_n - \sum_i^n y_i
\end{bmatrix}^T
\end{equation}

Conveniently, this is just $nCY$ for centering matrix $C$ and can be calculated in linear time with respect to $n$. This means that $nCY$ is just the per-point scaling based on that point's deviation from the mean.

Given this, our next step is to determine how this will interact with low-rank approximations of $G_X$ and $G_Y$. Plugging back in, our gradient is
\[ \nabla g_{pca}(X, Y) = 2n(G_X - G_Y)CY \]
where the gradient with respect to point $y_i$ has the form
\[ n \cdot \sum_{j = 1}^n \left[ \left( \langle x_i, x_j \rangle - \langle y_i, y_j \rangle \right) (y_j - \mu_y) \right] \]

Looking ahead, we make a prediction regarding the feasability of approximating this.

Given an arbitrary row (or column) $i$ of $G_X$ or $G_Y$, we see that each element in it is scaled by the norm of $y_i$. A low-rank approximation, then,
would inherently be biased towards preserving those rows and columns whose diagonal entries are large. Indeed, this is specifically the property that the
sublinear-time approximation algorithm exploits. We have the added component, however, of multiplying by $CY$, where each element $i$ is also linearly dependent
on the norm of $y_i$.
Our gradient's $i$-th element, then, should have a quadratic relationship with respect to $||y_i||$. Since this is what the low-rank approximation will
attempt to preserve, intuition implies that our element-wise approximation factor should not worsen significantly when we multiply $(G_X - G_Y)$ by $(CY)$.

\subsection{Gradient approximation}
tbd

\subsection{Bringing it back to the original problem}
At first glance, reintroducing the double centering operation into the gradient calculation seems to complicate matters. First, it doesn't appear
that double centering the Gram matrices retains their positive definiteness, so we cannot apply the low-rank approximation to their centered counterparts. This
implies that the centering must be done on the low-rank approximations themselves. Second, exactly calculating the double centering is an $O(n^2)$ operation.

This may be straightforward to speed up, however, as we know that the maximum value of a PSD matrix must occur along the diagonal. Furthermore, we know by
Cauchy-Schwarz that the sum of the norms of the diagonals is an upper bound on the Frobenius norm of a PSD matrix. So we have clear bounds that guide
our sampling for row- and column-means.

\end{document}
