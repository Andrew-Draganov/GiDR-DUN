\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[left=1in, right=1in, top=1in, bottom=1in]{geometry}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{changepage}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage[backend=bibtex]{biblatex}
\theoremstyle{definition}
\newtheorem*{definition}{Definition}
\allowdisplaybreaks
\title{PCA as Gradient Descent}
\author{Andrew Draganov}

\begin{document}
\maketitle
\section{PCA objective function and gradient}

\begin{figure}
\centering
\includegraphics[width=.4\textwidth]{tex_images/0500.png}
\caption{The result of performing gradient descent PCA on the MNIST dataset}
\end{figure}

Given a high-dimensional dataset $X \in \mathbb{R}^{N \times D}$, we aim to identify a gradient descent objective to find points $Y \in \mathbb{R}^{N \times d}$
with $d << D$ such that $Y$ is the result of performing PCA on $X$.

We start with formalizing PCA as minimizing the function

\begin{equation}
    f_{PCA}(X, Y) = ||L (E^X - E^Y) L ||^2_F
\end{equation}
where $E^X \in \mathbb{R}^{N \times N}$ and $E^Y \in \mathbb{R}^{N \times N}$ are the pairwise squared distance matrix for $X$ and $Y$ and $L$ is the $N \times N$ centering matrix. Optimizing this function inherently
amounts to minimizing the Frobenius norm between the two matrices.

To identify the effect of gradient descent on $Y$, we deconstruct the matrices into a set of element-wise operations. We can re-arrange the term in the
Frobenius norm:
\begin{align*}
    L (E^X - E^Y) L &= LE^XL - LE^YL \\
    &= E^X - \bar{X}^{\rightarrow} - \bar{X}^{\downarrow} - E^Y + \bar{Y}^{\rightarrow} + \bar{Y}^{\downarrow}
\end{align*}
where $\bar{X}^{\rightarrow}$ is the $N \times N$ matrix of row-means that the centering matrix on the right-hand-side of $X$ subtracts. The other $\bar{X}$ and
$\bar{Y}$ variables are defined accordingly. We can now rearrange these to obtain:
\[ L (E^X - E^Y) L = E^X - E^Y - \Lambda \]
for $\Lambda = - \bar{X}^{\rightarrow} - \bar{X}^{\downarrow} + \bar{Y}^{\rightarrow} + \bar{Y}^{\downarrow} \in \mathbb{R}^{N \times N}$

Then the PCA objective function can equivalently be written as
\begin{equation}
    f_{PCA}(X, Y) = \sum_{i, j} \left( ||x_i - x_j||^2 - ||y_i - y_j||^2 - \lambda_{ij} \right) ^2
\end{equation}

To perform gradient descent, we take the partial derivative with respect to $y_i$ to obtain
\[ \nabla_{y_i} f(X, Y) = -4 \sum_{j} (||x_i - x_j||^2 - ||y_i - y_j||^2 - \lambda_{ij}) (\vec{y}_i - \vec{y}_j) \]

This gives us the following gradient descent algorithm with learning rate $\nu$:
\begin{equation*}
    \boxed{
    \begin{aligned}
        \nabla_{y_i} f(X, Y) &= -4 \sum_{j} ( ||x_i - x_j||^2 - ||y_i - y_j||^2 - \lambda_{ij}) (\vec{y}_i - \vec{y}_j) \\
        Y_{t + 1} &= Y_t + \nu \nabla_{Y} f(X, Y)
    \end{aligned}
    }
\end{equation*}

Notice that we only use the off-diagonal elements of the pairwise relationships, as both $E^X$ and $E^Y$ are zero on the diagonal.

Lastly, we note that we can separate the PCA gradient descent algorithm into a pair of alternating forces. For $\Lambda^X = \bar{X}^{\rightarrow}
+ \bar{X}^{\downarrow}$ and $\Lambda^Y = - \bar{Y}^{\rightarrow} - \bar{Y}^{\downarrow}$ Namely, we have
\begin{align*}
    \nabla_{y_i} f(X, Y) &= -4 \sum_{i, j} ( ||x_i - x_j||^2 - ||y_i - y_j||^2 - \lambda_{ij}) (\vec{y}_i - \vec{y}_j) \\
    &= 4 \sum_{i, j} (||x_i - x_j||^2 - \lambda^X_{ij}) (\vec{y}_i - \vec{y}_j) - 4 \sum_{i, j} (||y_i - y_j||^2 - \lambda^Y_{ij})(\vec{y}_i
    - \vec{y}_j) \\
    &= 4 \left( F_{X} - F_{Y} \right)
\end{align*}
Each force is dominated by the large and small distances. However, we can deconstruct these into clear attractive and repulsive forces. If points $x_i$
and $x_j$ are one another's nearest neighbors in the high-dimensional space, then $||x_i - x_j||^2$ will always be less than its respective centering
$\lambda^X_{ij}$. Thus, nearest neighbors in the high-dimensional space \textit{only} exert attractions on the embedding. Similarly, farthest
neighbors in the high-dimensional space \textit{only} create repulsive forces. Inversely, nearest neighbors in the embedding exert repulsions while
farthest neighbors exert attractions. PCA is the single convergence of these individual systems. Therefore, as one progresses from $x_i$'s nearest to its farthest neighbor, the force must this neighbor exerts on $x_i$ must change monotonically.

\end{document}
